[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Decentralized Finance and Blockchains",
    "section": "",
    "text": "About",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "index.html#course-description",
    "href": "index.html#course-description",
    "title": "Decentralized Finance and Blockchains",
    "section": "Course description",
    "text": "Course description\nWelcome to the course “Decentralized Finance and Blockchains”!\nThis course approaches decentralized finance and blockchains from three different angles: the computer science or cryptography angle (CS), the game-theoretic or incentives angle (GT), and the financial economics angle (FE).\nThe three different parts are taught by three different lecturers: Sander Gribling (CS), Pieter Kleer (GT), and Nikolaus Schweizer (FE). Likewise, these lecture notes are also split into three parts. Below, you can find a brief description of each of the three angles, as well as the learning goals of the course.\nComputer Science: A key principle underlying blockchain technology is maintaining consensus about a distributed ledger, the archive of past transactions. We will study the security aspects related to establishing consensus. We discuss various (im)possibility results in the presence of malicious agents (Byzantine Fault Tolerance) in the general setting of distributed networks. We then discuss the mathematical models behind two famous mechanisms to maintain consensus: Proof of Work and Proof of Stake.\nGame Theory: Game theory and mechanism design play an important role in the analysis and design of decentralized financial protocols such as those building on blockchain technology. Prominent examples here are cryptocurrencies like Bitcoin. We will be studying such protocols from a game-theoretical perspective by looking at equilibria of their mathematical description, as well as various mechanisms that are used to guide those systems to the desired outcomes.\nFinancial Economics: One of the most intriguing capabilities of the blockchain technology are so-called smart contracts, computer programs that run on the blockchain in a transparent and decentralized manner, thus providing the basis for decentralized finance, the creation of decentralized counterparts to traditional financial institutions. In recent years, a particular success have been decentralized exchanges such as Uniswap which run in the blockchain and replace the traditional market maker and order book of a centralized exchange with an automated market maker. In the course, we will study and compare both types of exchanges using tools from the classical theory of market microstructure and analyze how different aspects of their design affect the functioning of the resulting financial market.",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "index.html#learning-goals",
    "href": "index.html#learning-goals",
    "title": "Decentralized Finance and Blockchains",
    "section": "Learning goals",
    "text": "Learning goals\nAfter successful completion of this course, you are able to:\n\nmodel blockchain technologies from the perspective of distributed computing and prove the (im)possibility of Byzantine Fault Tolerance under various assumptions.\nexplain consensus protocols in distributed networks and compute the probability that malicious agents can change the outcome (e.g. for the Dolev-Strong and longest-chain consensus protocols).\nmodel decentralized financial protocols from a game-theoretical perspective and compute their equilibria.\nexplain how mechanism design and game theory can be used to create the desired incentives in decentralized systems and compute the outcome of the relevant mechanisms.\nanalyze both centralized and decentralized financial exchanges using tools from market microstructure theory.\nexplain how the design of a financial exchange affects the properties of the resulting financial market regarding, e.g., liquidity and absence of arbitrage.",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "index.html#acknowledgments",
    "href": "index.html#acknowledgments",
    "title": "Decentralized Finance and Blockchains",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nThe first part of these lecture notes, the computer science perspective, is heavily inspired by a course taught by Tim Roughgarden called “Foundations of Blockchain Protocols”. You can find this course here: Course website, Lecture notes.\nNote that Roughgarden’s course is geared towards computer science students. The expected prior knowledge is thus very different compared to this course. The scope of Roughgarden’s course is therefore also much larger than ours: it covers the computer science angle in much more depth than we do here. If you want to learn (much) more about the CS perspective, this is a great starting point.\nFor full disclosure, the Python code snippets were sometimes/often generated using ChatGPT.",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "index.html#contact-information",
    "href": "index.html#contact-information",
    "title": "Decentralized Finance and Blockchains",
    "section": "Contact information",
    "text": "Contact information\nLecturers:\n\nSander Gribling\nPieter Kleer\nNikolaus Schweizer (course coordinator)\n\nNote that this is a new course, it is likely that we can still improve the clarity of the exposition. We therefore welcome constructive feedback on these lecture notes either via email, Canvas, or in class.",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "01-introduction.html",
    "href": "01-introduction.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 A computer science perspective\nThis course is about the science behind blockchain protocols and their applications. The most famous applications being of course cryptocurrencies such as Bitcoin.\nLet us first make a disclaimer: in this you will not learn anything about trading in cryptocurrencies. Instead, you will (hopefully) learn the principles behind blockchain protocols and the problems that frequently arise in applications.\nAt this point, you probably have some rough idea of what a blockchain is and how cryptocurrencies like Bitcoin work, perhaps based on reading some news articles or watching a short video on social media. For the purpose of this course, it is useful to have the following (simplified) picture in mind.\nThe precise meaning of decentralized is something that we will explore in this course. For now, imagine that it means that every participant in the (crypto)currency knows the exact amount on every other bank account, as well as a full list of the transactions that have ever taken place.\nWe often think about the set of participants as a network consisting of nodes and edges. The nodes represent the participants (we sometimes also referred to participants as parties). The edges represent communication links between two parties. Figure 1.1 below shows an example of a network with three nodes, who can all communicate with each other.\nIn cryptocurrency applications, it is often convenient (and realistic) to assume that every node can communicate with every other node. This means that all edges are present. This is however not a realistic assumption in every application: if the nodes represent people at a Dutch birthday party for example, then they are likely seated in a circle and can only communicate to their nearest neighbors. While this might seem like a silly example, the resulting network is a common toy model with interesting properties that we will revisit later on.\nEvery node maintaining a complete list of all bank accounts and transactions might seem like a lot to ask for, and indeed, it is. It is however a useful picture to have in mind. It for example requires us to solve the following problems:\nIt is not hard to imagine that it is a lot of work to maintain knowledge in a decentralized fashion, indeed, it will take us several chapters to do so! A natural question is therefore, how do we incentivize parties to perform this work? If you have ever read a popular science article about Bitcoin, you might have heard the terms “Proof of Work” or “Bitcoin mining” (if not, we will get to this in Chapter 6). These are examples in which parties perform some action (work, e.g., mining a Bitcoin) and are rewarded for this work. Often, the task is too large to perform by a single party and therefore parties form coalitions. How do we divide rewards in this case? This is one of the topics covered in Part II.\nOnce one has settled on the technology and protocols behind a blockchain-based currency, we can turn to applications. This is what Part III is all about.\nIn the remainder of this introductory chapter, we will give a high-level overview of the three different perspectives on blockchains and their applications that we will discuss in this course. At a first reading, we do not expect you to understand everything in these sections. As the course progresses, you should be able to answer more and more of the questions raised in these sections. (Let us know if we forgot to answer some!)\nIn this part of the course we will focus on the science behind blockchain protocols. Let’s start by unpacking the terminology “blockchain protocols” a bit.\nThe “protocols” here refers to a set of instructions that all participants have to follow. To make it concrete, you think of it as piece of code (a computer program) that all parties have to execute.1 Such a protocol is designed to perform a certain task. In this course, we will focus on what we want a protocol to do, rather than how to implement it on a computer. (In other words, there will not be much coding in this course.) In the case of blockchain protocols, the task is to ensure that all parties (eventually) agree on what is written in the blockchain. This last sentence is intentionally a bit vague, we will be more precise later on about what we mean by “eventually” and “agree”.\nA “blockchain” simply refers to a chain of blocks. The blocks can be used to store information; again, we will be more precise about which kind of information later on. We want to think of a block all information that is available at a certain moment in time. Logically, we would then like to connect a block to the block that preceded it in time. We can thus think of a chain of blocks as a special kind of network. As opposed to the networks that we described before, a chain requires a directed network. What do we mean by directed? In our previous description of a network, an edge represented a “communication link” and we implicitly assumed that if, say, Alice can communicate with Bob, then Bob can also communicate with Alice. In a directed network the direction of edges matters. Visually, we will represent this using arrows. If there is an edge between nodes 2 and 1, then we draw an arrow from node 2 to node 1. Here is an example of a chain of three blocks.\nWe will work towards an understanding of blockchains gradually. We will first get familiar with computation in a decentralized setting. To start, we should decide on a mathematical model for the decentralized setting (or several models). Here one can think for example about questions such as how do we model communication? Is communication instantaneous (the synchronous model), or can there be delays (an asynchronous model)? Do we know all participants in the network in advance or can anyone participate? All of these lead to valid models! We will therefore be explicit about which assumptions we are making. When we make an assumption, we should always ask ourselves whether it is a reasonable or realistic assumption. Whether an assumption is realistic or not often depends on the application (instantaneous communication anyone?). Exploring what is and what is not possible under a given set of assumptions is essentially the first part of the course.\nTo be more specific, we will focus on something we have touched upon before: “agreeing” on information. In the literature we often refer to reaching agreement as “building consensus”. You might see “Proof of Work” and/or “Proof of stake” referred to as consensus-building protocols. The concept of consensus is however separate from blockchains and indeed much older. At this point you should wonder, if everyone can communicate with everyone and we are all following the same protocol, how can we ever disagree? This is a very good question! The answer is that it is often very unreasonable to assume that everyone follows the same protocol! For example, if by deviating from the Bitcoin protocol you could earn a lot of money, then probably someone will decide to deviate. We thus need to design protocols that can resist such dishonest parties, allowing the honest parties to reach consensus. We will call dishonest parties “Byzantine agents” later on. One of the problems that we will discuss is the Byzantine broadcast problem.\nThe general area to which these questions belong is that of distributed computing. Distributed computing is much broader than what we can cover in this course. Nevertheless, after setting up the framework more formally, we will quickly be able to discuss some foundational results in distributed computing: the state machine replication problem, and the Dolev-Strong protocol for Byzantine broadcast. These are results about the synchronous setting, assuming we know the entire network in advance. Neither of these assumptions is very realistic for blockchains, but is a useful starting point due to its simplicity. It will allow us to quickly get our hands dirty, without getting lost in the details of blockchains.\nHaving said that, we then move to precisely this: the details of blockchain protocols. More precisely, we will discuss longest chain protocols as a general framework for building consensus in a network that we do not know in advance. We finally discuss in more detail one particular longest chain protocol: the one that is built on the concept of “Proof of work”, used for example in Bitcoin where “mining a Bitcoin” is a “proof of work”.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-introduction.html#a-computer-science-perspective",
    "href": "01-introduction.html#a-computer-science-perspective",
    "title": "1  Introduction",
    "section": "",
    "text": "Figure 1.2: A chain with three blocks",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-introduction.html#a-game-theory-perspective",
    "href": "01-introduction.html#a-game-theory-perspective",
    "title": "1  Introduction",
    "section": "1.2 A game theory perspective",
    "text": "1.2 A game theory perspective\nThis part of the course will be concerned with incentives and strategical aspects that may arise in blockchain protocols. We will illustrate these concepts here using using the example of “Bitcoin mining”. This is done by solving a complex mathematical puzzle that requires a lot of computation power and is typically done by multiple parties or miners.\nOnce a Bitcoin is mined, i.e., created, how do we split it fairly between the miners that were involved? A function that decides on the reward that every miner receives is called an allocation rule and can be defined in many ways. Ideally, we want the allocation rule to have some desirable properties that incentivize miners to act faithfully. One such property is sybil-proofness: A miner should not have an incentive to split itself up in multiple parties and receive, in total, more reward from the allocation rule then it would have received when participating as one single party or miner. Reversely, we would also like the allocation rule to be collusion-proof meaning that different miners should not receive more total reward in the allocation rule if they pretend to be one miner, as opposed to the sum of their individual rewards. Our goal here will be to understand and characterize which allocation rules satisfy these, and other, desirable properties by looking at this problem through the lens of cooperative game theory.\nOne can also take a more competitive view towards Bitcoin mining by considering it to be a so-called Tullock contest. Here every miner invests a certain amount of computing power, but instead of sharing the reward generated by mining a Bitcoin, the Tullock contest gives the whole Bitcoin to the first miner to solve the mathematical puzzle. The probability for a miner to win this contest depends on the amount of computing power that was invested. The miners have a strategic choice to make on how much computing power they want to invest, while optimizing their chance of winning the contest. Our goal will be to analyze this contest through the lens of non-cooperative game theory using stability concepts such as the Nash equilibrium.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-introduction.html#a-financial-economics-perspective",
    "href": "01-introduction.html#a-financial-economics-perspective",
    "title": "1  Introduction",
    "section": "1.3 A financial economics perspective",
    "text": "1.3 A financial economics perspective\nFrom a financial economics perspective, one of the greatest promises and challenges of the blockchain technology is the possibility of organizing counterparts to traditional financial institutions in decentralized ways. An important impulse for this development was the great financial crisis of 2008 when financial institutions that were considered “too big to fail” placed a huge burden on societies and created considerable distrust in traditional “centralized” finance. After the invention of bitcoin, a second important step towards decentralizing financial institutions was the invention of so-called smart contracts, computer programs that can be embedded in later-generation blockchains like Ethereum and that can be used to create automated protocols for financial transactions.\nSince the advent of smart contracts, people have looked into various ways of creating decentralized counterparts to traditional financial institutions using smart contracts. Yet there are challenges, some of them as old as financial markets, some of them rather new. Think of decentralized car insurance, implemented as a computer program that automatically sends you an agreed upon amount of cryptocurrency when you upload a photo of your damaged car. What could possibly go wrong? There is more than one answer to this question. Let us just say that, traditionally, most successful insurance markets have operated in environments with a well-functioning legal system in the background.2\nAutomated market makers such as Uniswap are one of the most successful developments in decentralized finance, exhibiting tremendous trading volumes. These are automated exchanges that enable market participants to exchange assets with an automated mechanism that was inspired by the way sports betting markets are organized. Again, traders in these exchanges face some of the same challenges seen in traditional financial exchanges. Yet, there are also new challenges. For instance, due to the full transparency and the block structure of transactions, submitted orders can be seen by others before they are executed – and the order of submissions is not necessarily the order of executions, creating all sorts of potential problems.\nThe goal in the financial economics part of the course is to get some understanding of the relevant decentralized financial institutions and then take some steps towards understanding them even better by applying models from quantitative finance.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-introduction.html#footnotes",
    "href": "01-introduction.html#footnotes",
    "title": "1  Introduction",
    "section": "",
    "text": "For example, look at all information currently available to the participant, do some computation, and report the outcome to all neighboring participants.↩︎\nConversely, in general, the potential benefits of decentralized financial institutions may be greatest in environments without well-functioning legal systems, where citizens have to be concerned that those in power confiscate their traditional bank accounts and so forth.↩︎",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "prelim.html",
    "href": "prelim.html",
    "title": "2  Preliminaries",
    "section": "",
    "text": "2.1 Commonly used symbols\nHere we gather frequently used notation. We also recall some basic concepts that we expect as prior knowledge and we provide some pointers to related literature.\nWe let \\mathbb{N} denote the set of natural numbers, i.e., \\mathbb{N} := \\{0,1,2,3,\\ldots\\}. We often use for example n \\in \\mathbb{N} to mean “n is a natural number” (it is also sometimes implicitly assumed that n is larger than 0).\nFor n \\in \\mathbb{N}, we write [n] := \\{1,2,\\ldots,n\\}.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Preliminaries</span>"
    ]
  },
  {
    "objectID": "prelim.html#graph-theory-preliminaries",
    "href": "prelim.html#graph-theory-preliminaries",
    "title": "2  Preliminaries",
    "section": "2.2 Graph theory preliminaries",
    "text": "2.2 Graph theory preliminaries\nWe will use the concepts of undirected and directed graphs, which we recall below. Add reference to some book\nA graph G is defined by a set of vertices V and a set of edges E. We write G = (V,E). Here the set of edges is a subset of pairs of vertices. Throughout, we assume an edge consists of a pair of distinct vertices u,v \\in V (distinct meaning u \\neq v). When such an edge is undirected we write \\{u,v\\}. When the edge is directed we write (u,v) to denote an edge from vertex u to vertex v. We have seen an example of an undirected graph in Figure 1.1: this is the graph with vertex set V=[3] and edge set E = \\{\\{1,2\\},\\{2,3\\},\\{1,3\\}\\}.\nWe have also seen an example of a directed graph in Figure 1.2: here the vertex set is again V=[3] and if we label the vertices 1, 2, and 3 from left to right, then the edge set is E = \\{(2,1), (3,2)\\}.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Preliminaries</span>"
    ]
  },
  {
    "objectID": "prelim.html#frequently-used-probability-distributions",
    "href": "prelim.html#frequently-used-probability-distributions",
    "title": "2  Preliminaries",
    "section": "2.3 Frequently used probability distributions",
    "text": "2.3 Frequently used probability distributions",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Preliminaries</span>"
    ]
  },
  {
    "objectID": "distributed.html",
    "href": "distributed.html",
    "title": "3  Introduction to distributed computing",
    "section": "",
    "text": "3.1 Example: maintaining copies of a file\nIn this chapter we will define some basic concepts and problems from the area of distributed computing.\nWe start this chapter with an example of a common distributed computing problem: that of making backups of a database and ensuring that these backups are synchronized. This is a problem that you have probably struggled with at some point: how do you properly backup the files on your hard disk? To look ahead, the problem that we aim to solve when we want to base cryptocurrencies on blockchains is very similar: the database would correspond to a complete list of the transactions that have ever occurred.\nWe then give a formal definition of the underlying problem: the State Machine Replication (SMR) problem. The second half of this chapter is devoted to formalizing and discussing various assumptions about distributed computing networks. The next chapters are dedicated to solutions of the SMR problem under increasingly more realistic assumptions.\nTo set the stage, consider the following situation. On my computer, I have an excel file with the grades of all students who take this course. Naturally, I would like to make sure that I don’t lose this file. To do so, I can create multiple copies of this file and store them on different computers. That way, if one of the computers breaks, I still have a copy of the file on another computer. Making such a backup once, is easy enough. The problem that we will consider arises when I want to be able to update the file (for example, after the resit) in such a way that the various backups to agree with each other.\nWe start to see an outline of a distributed computing problem: we can view the different computers as nodes in a graph. If there are n computers, there would be n nodes in the graph. Our task would be to ensure that the n computers each have an up to date copy of the file.\nThe above example is written from a centralized perspective. There is one agent, the “I” persona, that can simply perform the update to each copy of the file. In a decentralized setting, we would like the computers to run some sort of protocol that ensures that if I change the file on one of the computers, then the same change is made on all other devices. One quickly realizes that this protocol will require the devices to communicate. This brings us to the second part of the graph: the communication links between the computers determine the edge set of the graph.1\nConnection to blockchains: If we want to construct a cryptocurrency based on blockchains, then we are essentially interested in solving a similar type of problem. Indeed, we can draw the following analogy. Each participant in the cryptocurrency would correspond to a node in the network. The file that we want each participant to maintain consists of two parts: a list of the current balance of each account, and a full history of all transactions that have ever taken place between participants. In this variant of the problem, there is no natural “I” persona. The decentralized setting is built in: we want each of the nodes to be able to change the file. We do of course want all the nodes to agree on the same file! A change made by one of the nodes should be replicated by all other nodes.\nIn the next section we will formally define a generalization of the above examples.",
    "crumbs": [
      "Computer Science",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to distributed computing</span>"
    ]
  },
  {
    "objectID": "distributed.html#state-machine-replication-problem",
    "href": "distributed.html#state-machine-replication-problem",
    "title": "3  Introduction to distributed computing",
    "section": "3.2 State Machine Replication problem",
    "text": "3.2 State Machine Replication problem\nLet us now give a formal definition of a problem that encompasses both examples from the previous section. In the distributed computing literature this problem is known as the State Machine Replication (SMR) problem. To explain the terminology: the state of the machine corresponds to the file that we wanted to maintain in the previous examples.\nIn the SMR problem we consider the following:\n\nThere is a set of nodes responsible for running a consensus protocol, and a set of clients who may submit “transactions” to one or more of the nodes.\nEach node maintains a local file that we will call its history.2\nNodes can send messages to other nodes, and receive messages from other nodes.\n\nSome remarks are in order. We differentiate here between nodes and clients. The nodes are responsible for maintaining copies of the file. The clients may suggest modifications to the file by sending messages (instructions) to the nodes. For simplicity we assume here that modifications consist of adding information to the file (so no deletions). In a cryptocurrency application this is a reasonable assumption: the file – or history – represents the history of all transactions, which can only grow over time.\nInformally, the SMR problem asks to keep all the nodes in sync. Meaning that the local histories of all nodes are the same.\nMore formally, the SMR problem is to design a protocol that is to be executed by each of the nodes. (Think of a piece of code.) The protocol is allowed to do the following operations:\n\nmaintain or change the local state of the node,\nreceive messages from other nodes and from clients,\nsend messages to other nodes.\n\nWe will see examples of protocol shortly. First, we need to define the properties that we want the protocol to have. In other words, how do we formalize “maintaining a file”? Here we can distinguish two key properties. The first is a safety guarantee: we want all nodes to agree on the same file. The second is a liveness guarantee: we want to be able to modify the file.\nGoal 1: Consistency. We say that a protocol satisfies consistency if all the nodes running it always agree on the history. That means that the local history of all the nodes is equal.\nIn particular, in the case where the local history is supposed to be a list of transactions, all nodes would agree on the order of the transactions.\nGoal 2: Liveness. Every “transaction” submitted by a client to at least one node is eventually added to every node’s local history.\nFor the moment, we view “transactions” as simply adding an entry in the file. That is, we ignore the very important financial question of whether the transaction is “valid” – agreeing with the current balance in each of the accounts.\nThe two goals together are non-trivial to satisfy. It is however not so hard design protocols that reach exactly one of the two goals.3\n\n3.2.1 Protocol achieving consistency\nHere is our first protocol. It will achieve consistency, but not liveness. We will describe the protocol by giving its pseudocode. That is, we describe the protocol mostly in words, without committing to a specific programming language.\nAlg_Consistency: \n1.  Initialize: local history H = [ ].\n2.  Upon receiving a message m from a client do:\n        Nothing.\n3.  Upon receiving a message m from a node do:\n        Nothing.\nAs you can see, the protocol consists of three lines. The first line describes the initialization that the node performs. In this case, it defines its local history H to be the empty list H = [\\ ]. The second line describes the behavior of the node when it receives a message from a client. The third line does the same for when it receives a message from a node. Here the last two actions are rather trivial: do nothing.\nAlthough this is an extremely naive protocol, we can show that it does satisfy the first goal: consistency.\n\nLemma 3.1 If all nodes in a distributed network run Alg_Consistency, then it satisfies consistency.\n\n\nProof. Assume all nodes in a distributed network run Alg_Consistency. To argue that we satisfy consistency, we observe that for each node and every moment in time the local history state equals the empty list [\\ ]. In particular, this means that all nodes agree on the same local history, at all times.\n\nYou are recommended to think about the following exercise before proceeding to the next section.\n\n\n\n\n\n\nExercise\n\n\n\nConvince yourself that a distributed network whose nodes all run Alg_Consistency does not guarantee liveness.\n\n\n\n\n3.2.2 Protocol achieving liveness\nAs a second example, we will give a protocol that achieves liveness, but not necessarily consistence.\nAlg_Liveness: \n1.  Initialize: local history H = [ ].\n2.  Upon receiving a message m from a client do:\n        Append message m to H. \n3.  Upon receiving a message m from a node do:\n        Append message m to H.\n4.  At midnight do:\n        Send local history H to all other nodes.\nA couple of remarks are in order. First, the protocol now actually “does something”! In particular, a node will act upon a message that it receives from either a client or another node. Second, we see that nodes are no longer passive: they occasionally send messages to other nodes as well. Third, the protocol now – implicitly – assumes that a node is aware of the concept of time: it needs to perform a specified action every day at midnight. This is in stark contrast to the protocol Alg_Consistency which was purely event-driven: a node only had to take action when a message arrived. (“Took action” is maybe a slight exaggeration: the protocol doesn’t do anything when a message arrives.) For the purpose of this example, we will assume that all nodes agree on the current time. When an event happens once per day, this is a relatively mild assumption. It also means that messages from clients only get shared once per day, which might not be sufficiently quick depending on the application. At the other extreme, we could imagine the nodes want to share incoming messages every millisecond. In that case however, you might run into all kinds of issues: nodes might be too far apart for a message to pass from node A to node B within that time frame, or nodes might disagree on the current time (we are now measuring milliseconds after all). In that case, agreeing on the time is a much stronger assumption. We will revisit these – and other – assumptions in more detail later on. For now, let us prove that Alg_Liveness guarantees the liveness property.\n\nLemma 3.2 If all nodes in a distributed network run Alg_Liveness, then it satisfies liveness.\n\n\nProof. Assume all nodes in a distributed network run Alg_Consistency. To argue that we satisfy liveness, we need to show that if a client sends a message m to one or more nodes in the network, then it eventually gets added to the local history of every node. To that end, assume client i sends a message m to a group of nodes that includes node A, on day 1. On day 1 node A receives message m and adds it to its local history H_A. Now consider an arbitrary node B in this network that is distinct from A. (It might have received message m from client i on day 1 as well, in which case it added m to H_B on day 1 and there is nothing left to show.) At the end of day 1, node A sends their local history H_A to all other nodes. Therefore, on day 2, node B receives H_A and appends it to H_B. Since H_A included the message m, this means that H_B now contains the message m as well. Thus, the message m is eventually added to the local history of every node.4\n\nIt is important to note however that Alg_Consistency does not guarantee the consistency property. Can you see why?\n\n\n\n\n\n\nExercise\n\n\n\nConsider a distributed network containing two nodes A and B where on day 1 client i sends message m_A to A and j sends message m_B to B. Describe the local history of each of the nodes on days 1 and 2. What do you observe?",
    "crumbs": [
      "Computer Science",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to distributed computing</span>"
    ]
  },
  {
    "objectID": "distributed.html#strong-assumptions-about-the-decentralized-setting",
    "href": "distributed.html#strong-assumptions-about-the-decentralized-setting",
    "title": "3  Introduction to distributed computing",
    "section": "3.3 (Strong) assumptions about the decentralized setting",
    "text": "3.3 (Strong) assumptions about the decentralized setting\nWe will be working with a mathematical model of a real-world situation. We are therefore making some assumptions. In this section we list one possible set of assumptions. Some of the assumptions that we are making here are more restrictive than others. We will call such an assumption relatively strong. In later chapters we will replace these strong assumptions with weaker assumptions. While reading the next three assumptions, try to answer the following questions: is it a weak or strong assumption? Do I know an application where it holds / does not hold?\n\n3.3.1 Assumption 1: the set of nodes is known\nThis assumption is also referred to as the permissioned setting. It assumes the set of nodes in the network is fixed and known to all nodes, moreover it assumes that each node has a unique identifier that is also known to all other nodes. We will frequently use n \\in \\mathbb{N} to denote the number of nodes. This allows us to use the numbers between 1 and n as unique identifiers.\nKey advantages:\n\nIt allows majority voting.\nOne can order the nodes based on their identifier.\n\n\n\n3.3.2 Assumption 2: signatures exist and cannot be forged\nThis assumption can be viewed as an extension of Assumption 1. We assume that nodes can add their signature to a message. By this we mean that if node A sends a message m (to an arbitrary node), then they can add their signature to it. All other nodes have a verification procedure that can correctly determine whether node A signed message m. No other node can forge A’s signature: no other node can add a ‘signature’ that the verification procedure would accept as A’s signature. This is an example of a trusted setup. This assumption is also referred to as assuming Public Key Infrastructure (PKI).\nWe will not go into further details about this assumption in this course; we will (happily) assume that it holds and not go into details of how one would implement it in a real-world scenario. (It is a relatively mild assumption however.)\nKey advantage:\n\nIt allows us to trust who sent which message.\n\n\n\n3.3.3 Assumption 3: synchronous model\nThis is an assumption about the (reliability of the) communication network. Formally, we require the following two sub-assumptions:\n\nAll nodes have access to a shared clock.\nBounded message delays: messages arrive within a predetermined amount of time.\n\nTogether, these two assumptions allow us to divide time into smaller intervals in such a way that that messages sent at the start of an interval arrive before the end of the interval. To avoid having to specify the bounded message delay, we will simply number the intervals. Concretely, we thus assume that messages sent at the start of (or simply in) interval t arrive at their intended recipient before the start of interval t+1. We will often refer to the intervals as rounds.\nKey advantage:\n\nIt allows us to define rounds (see above).\n\n\n\n3.3.4 Discussion of the assumptions\nAssumption 1 is realistic in some scenarios: if we are using multiple computers to create a backup of a file (e.g. a list of grades), then it reasonable to assume that we know how many computers we are going to use. (There is a central entity that determines the number of nodes.) For our second motivating example however, blockchains, this is a very unrealistic assumption! Not knowing the set of nodes participating in the blockchain protocol is in fact a key feature that we are aiming for. We would like a blockchain protocol to be able to function in a completely decentralized manner, with nodes being able to enter (or leave) the network while the protocol is active.\nAssumption 2, as mentioned above, is one that we will simply assume throughout the course.\nAssumption 3 is again realistic in some scenarios, but not in others. We have seen an example of a protocol that worked in the synchronous model in Section 3.2.2: the Alg_Consistency protocol. The synchronous model makes optimistic assumptions and therefore serves as a good sanity check when designing protocols: the protocol should at least function correctly in the synchronous model. In Chapter 4 we will work with the synchronous model. In the later chapters Chapter 5 and Chapter 6 we will encounter protocols that make milder assumptions.",
    "crumbs": [
      "Computer Science",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to distributed computing</span>"
    ]
  },
  {
    "objectID": "distributed.html#sec-rotating",
    "href": "distributed.html#sec-rotating",
    "title": "3  Introduction to distributed computing",
    "section": "3.4 Honest nodes",
    "text": "3.4 Honest nodes\nThe final assumption is about whether we assume nodes to be ‘honest’ or ‘dishonest’. We say that a node is honest if it executes the intended protocol. Any node that deviates from the intended protocol is called dishonest or faulty. Note that honesty in this context is a description of the nodes behavior, not its intentions. In the example of creating backups of a list of grades, it is for example perfectly reasonable to assume that all nodes have good intentions, but we would like a protocol to `work well’ even if one of the nodes breaks and is therefore unable to follow the protocol. We therefore prefer the term faulty for nodes that are not honest.\nThe assumption that we will make in this section (and only this section) is a very strong one:\nAssumption 4: All nodes are honest.\nNaturally, we would like to relax this assumption as soon as possible. In the next chapter we will indeed replace it with a much milder assumption: there we assume a bound f on the number of faulty nodes.\n\n3.4.1 Solving the SMR problem under assumptions 1,2,3,4\nHere we show how to solve the SMR problem under the assumptions 1, 2, 3, and 4. That is, we work in the permissioned, synchronous model, we assume PKI and that all nodes are honest.\nAs a reminder, we want to design a protocol that guarantees consistency and liveness for the SMR problem. We have already seen two protocols that achieve either consistency or liveness. In particular, in Lemma 3.2 we have shown that Alg_Liveness guarantees the liveness property. In the subsequent discussion we have seen that this protocol does not guarantee consistency: it can happen that two nodes disagree on the order of transactions in their local history. The ‘issue’ here was that every node had the ‘right’ to append transactions to the local history of other nodes, which could lead to disagreements on the order that transactions are written down. The protocol that we describe here resolves this issue by selecting a leader in each round, who is the only one with the permission to write in that round.\nCoordinating via rotating leaders: since we are in the permissioned setting, we know the number of nodes participating in the network, say n. We can there do the following:\n\nin round 1, node 1 is called the leader and all other nodes are called followers,\nin round 2, node 2 is called the leader and all other nodes are called followers, …\nin round n, node n is called the leader and all other nodes are called followers After n rounds, we reset the clock and start again as in round 1. In other words, we are rotating the leaders.\n\nWe now describe the protocol for the leader and follower nodes separately. For ease of notation, we always assume that nodes initialize their local history to H = [\\ ] at the start of the protocol. We additionally allow the nodes to use some local workspace W, which they can use to store information (temporarily); it is not part of the local history state.\nAlg_Leader(t): \n1.  Upon receiving a message m from a client do:\n        Append message m to the local workspace W. \n2.  At the end of the round do:\n        Remove from W the messages that are already part of H. \n        Append W to H and send W to all other nodes. \n        Reset W = [ ].\nAlg_Follower(t): \n1.  Upon receiving a message m from a client do:\n        Append message m to the local workspace W. \n2.  Upon receiving a message m from a node do:\n        If m is signed by the leader of the current round t do:\n                Append m to H. \n        Else do:\n                Nothing. \nOur claim is that if the nodes in a distributed network adhere to the above protocol, then both liveness and consistency are guaranteed.\n\nLemma 3.3 If all nodes in a distributed network run the ‘coordinating via rotating leaders’ protocol, then it satisfies liveness and consistency.\n\n\n\n\n\n\n\nExercise\n\n\n\nProve Lemma 4.1.\n\n\nTo encourage you to attempt the exercise yourself first, the solution is “hidden” (in the HTML version). Click on the orange bar to show a possible solution.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe need to verify two properties: consistency and liveness. We first prove that consistency is guaranteed, using induction. Note that all nodes initialize to the same local history state, H = [\\ ]. Let us call this initialization phase round 0. Then consistency holds at round 0. Now, as our induction hypothesis, assume consistency holds for the first t-1 rounds. That is, all nodes agree on the local history state H during the first t-1 rounds. We show that consistency holds in round t as well. Indeed, in the t-th round the leader looks at their local workspace W, it removes all transactions that have previously been added to H, it then adds W to H and instructs all other nodes to do the same. (All other nodes indeed proceed to do the same.) This shows that all nodes agree on H at the end of round t as well.\nIt remains to argue that the protocol guarantees liveness. Suppose a client i sends a transaction m to at least one node. Since every node is elected as leader in one of the n nodes, the transaction m is eventually added to H.",
    "crumbs": [
      "Computer Science",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to distributed computing</span>"
    ]
  },
  {
    "objectID": "distributed.html#footnotes",
    "href": "distributed.html#footnotes",
    "title": "3  Introduction to distributed computing",
    "section": "",
    "text": "In this problem, it is natural to assume that communication works both ways. Therefore the graph would be undirected.↩︎\nIt represents for example an ordered list of transactions that only grows over time.↩︎\nWarning: these protocols might seem a bit silly, they are meant as an easy introduction to thinking about protocols.↩︎\nIn this case, eventually means at most one day after the client sends the message to at least one node in the network.↩︎",
    "crumbs": [
      "Computer Science",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to distributed computing</span>"
    ]
  },
  {
    "objectID": "DolevStrong.html",
    "href": "DolevStrong.html",
    "title": "4  The Dolev-Strong protocol",
    "section": "",
    "text": "4.1 Bounded number of faulty nodes\nIn the previous chapter we have seen a protocol for the SMR problem under the assumptions 1,2,3, and 4. Here we replace the last assumption by a more realistic one: we no longer assume all nodes are honest. Instead, we assume there are at most f faulty nodes in the network, where f is some number between 0 and n.\nOur goal in this chapter is to solve the SMR problem under the assumptions 1,2,3 and assuming a bound f on the number of faulty nodes (for some values f&gt;0). The protocol that we will study is due to Dolev and Strong (1983) TODO: add ref. At a high level, it works similarly to the rotating leaders protocol that we have seen in Section 3.4. If there is even a single faulty node, the rotating leaders protocol no longer satisfies consistency. Can you see why?1 Informally, the Dolev-Strong protocol gives us a way to detect faulty leaders that try to ‘trick’ the other nodes into inconsistency. To formalize this, we introduce the Byzantine Broadcast problem in Section 4.2. We show how the SMR problem reduces to the Byzantine broadcast problem, see Section 4.3. We finally discuss the Dolev-Strong protocol and show that it solves the Byzantine Broadcast problem.\nWe recall from the previous chapter that a node is called honest if it never deviates from the intended protocol. Any node that is not honest is called faulty. Assumption 4 from the previous chapter was that all nodes are honest, or, equivalently, that the number of faulty nodes is equal to 0. Here we replace this with a more realistic assumption:\nAssumption 4’: the number of faulty nodes in the network is at most f.\nWe in fact assume the protocol knows the upper bound f on the number of faulty nodes. This means the protocol may depend on f. In the previous chapter we have seen a protocol that assumed f=0. Interesting values of f to keep in mind are f = n/3 or f=n/2, meaning that at most a certain fraction of the nodes is faulty. To start building some intuition, we will consider f=1 and f=2 later on in this chapter.",
    "crumbs": [
      "Computer Science",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Dolev-Strong protocol</span>"
    ]
  },
  {
    "objectID": "DolevStrong.html#sec-BB",
    "href": "DolevStrong.html#sec-BB",
    "title": "4  The Dolev-Strong protocol",
    "section": "4.2 The Byzantine Broadcast problem",
    "text": "4.2 The Byzantine Broadcast problem\nIn the Byzantine Broadcast problem we consider the following setting:\n\nThere are n nodes, one is designated sender and the other n-1 nodes are non-sender. The identity of the sender is known to all non-senders.\nThe sender has a private input v^* that belongs to some set V. (Private means that only the sender knows v^* at the start of the protocol.)\n\nThe set V here represents the set of possible private inputs. In a cryptocurrency application, this might be the set of all valid transactions. For intuition, it suffices to think of only two possible inputs: V = \\{0,1\\}.\nInformally, the goal in the Byzantine broadcast problem is for the sender to send v^* to all other nodes in such a way that all other nodes can be certain that everyone received the same message. Formally, we say that a protocol is a solution to the Byzantine broadcast problem if it guarantees the following three properties:\n\nTermination: Every honest node i eventually halts with some output v_i \\in V.\nAgreement: All honest nodes halt with the same output.\nValidity: If the sender is an honest node, then the common output of the honest nodes is the private input v^* of the sender.\n\nSome remarks are in order. First, the requirements are different for honest nodes and faulty nodes. This is by necessity: faulty nodes can deviate in any way from the protocol, so we cannot hope to guarantee anything about their output. Second, the condition agreement is required to hold both if the sender is honest and if it is faulty. Agreement is comparable to the consistency property that we have seen in the SMR problem. Third, note that the validity property, necessarily so, is conditioned on the sender being honest. Therefore validity is trivially satisfied when the sender is faulty.\n\n\n\n\n\n\nExercise\n\n\n\nDesign a protocol, for any 0 \\leq f \\leq n specifying both the behavior of the sender and non-sender nodes, that guarantees termination and agreement whenever assumptions 1,2 and 3 hold.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nFix some v_0 \\in V (e.g. if V is a set of numbers, pick the smallest). Consider the simple protocol in which every node (both sender and non-sender) terminates in round 1 by outputting v_0. This protocol clearly satisfies termination (every node halts in round 1) and agreement (every honest node halts with the same output v_0).",
    "crumbs": [
      "Computer Science",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Dolev-Strong protocol</span>"
    ]
  },
  {
    "objectID": "DolevStrong.html#sec-SMR_reduction",
    "href": "DolevStrong.html#sec-SMR_reduction",
    "title": "4  The Dolev-Strong protocol",
    "section": "4.3 SMR reduces to Byzantine Broadcast",
    "text": "4.3 SMR reduces to Byzantine Broadcast\nHere we show that any protocol that solves the Byzantine Broadcast problem can be used as a black box to solve the State Machine Replication problem. In our original definition of the SMR problem, in particular for liveness and consistency, we assumed all nodes were honest. In the presence of faulty nodes we need modify the guarantees slightly:\nGoal 1: Consistency. We say that a protocol satisfies consistency if all the honest nodes running it always agree on the history.\nGoal 2: Liveness. Every “transaction” submitted by a client to at least one honest node is eventually added to every node’s local history.\n(The only difference is adding the word honest in the right places.)\nWe now present a protocol that solves SMR, given a protocol for the Byzantine Broadcast problem. Concretely, let us assume we are in the synchronous and permissioned setting (assumptions 1,2,3) and that there are at most f faulty nodes (assumption 4’). Moreover, assume we are given a protocol \\pi that solves the Byzantine Broadcast problem under those assumptions. Assume \\pi always terminates in at most T rounds. As before, we allow the nodes to use some local workspace W that they can use to store information, but which is not part of the local history state (this is used to implement step 2).\nAlg_SMR_from_BB(pi,f): \n\nAt each round t=0, T, 2T,... that is a multiple of T do:\n1.  Define the current leader to be node t/T modulo n. \n2.  The leader constructs a list L of transactions it has \n    received in the past T rounds, which are not yet part of H. \n3.  Use the protocol pi with \n    as leader node t/T modulo n and private input L. \n4.  At round t+T-1, every node i appends \n    its output L_i in the Byzantine Broadcast problem to its local history. \n\nLemma 4.1 Under assumptions 1,2,3,4’, assuming \\pi solves the Byzantine broadcast problem in at most T rounds, the protocol Alg_SMR_from_BB solves the SMR problem.\n\n\nProof. We need to argue that consistency and liveness are satisfied. For consistency, we can argue in an inductive manner. At the start of the protocol, all honest nodes initialize their local history to H=[\\ ]. Assume all honest nodes agree on the local history at some time that is a multiple of T. By the guarantees of \\pi, when \\pi terminates, which happens within T rounds, every honest node agrees on a common output L. Therefore all honest nodes append the same message L to their local history state H in round t+T-1, which ensures that the local history states of all honest nodes agree between rounds t and t+T.\nFor liveness, it suffices to observe that every honest node is elected as a leader once very nT rounds.",
    "crumbs": [
      "Computer Science",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Dolev-Strong protocol</span>"
    ]
  },
  {
    "objectID": "DolevStrong.html#the-cases-f1-and-f2",
    "href": "DolevStrong.html#the-cases-f1-and-f2",
    "title": "4  The Dolev-Strong protocol",
    "section": "4.4 The cases f=1 and f=2",
    "text": "4.4 The cases f=1 and f=2\nIn this section we introduce the idea of “cross-checking”. We show that this solves the Byzantine broadcast problem when there is at most 1 faulty node, and that it fails when there are 2 faulty nodes. This allows us to build op some intuition about the main idea underlying the Dolev-Strong protocol, without the technicalities that arise when there are multiple faulty nodes. Technically, the Dolev-Strong protocol that we present in the next section is independent from this section, which means that this section is “optional” and can be skipped (at your own risk).\nIntuitively, the protocol works by asking the honest nodes to do one simple step of cross-checking: each node verifies whether all other nodes received the same messages from the sender.\nFormally, we consider the following protocol.\nAlg_Cross_Check: \n\nThe protocol consists of three rounds:\n1.  The sender sends its private value v* to all non-senders (signed). \n2.  Every non-sender i sends the message m_i it \n    received from the sender in round 1 to \n    all other non-senders with their signature added.\n3.  All non-senders choose the most frequently \n    received value among the values it received in rounds 1 and 2. \n    (Breaking ties in some consistent way.)\n    The sender outputs v*.\nIn the following two exercises you are asked to show that Alg_Cross_Check solves the Byzantine broadcast problem when there is at most f=1 faulty node and there are at least 4 nodes in total, but that it breaks when f=2.\n\n\n\n\n\n\nExercise\n\n\n\nUnder assumptions 1,2,3, f=1, and n \\geq 4, the protocol Alg_Cross_Check solves the Byzantine broadcast problem.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nTermination is clearly satisfied. For validity, assume the sender is honest. In that case, after round 2 every honest non-sender has received the private input v^* of the sender at least n-2 times (from the other honest nodes) and has received a different output at most once (from the single faulty node). Since n-2&gt;1, the honest non-sender outputs v^* after round 3. The honest sender also outputs v^* and therefore all honest nodes agree agree on the common output v^*.\nThe above also shows that we satisfy agreement when the sender is honest. It thus remains to argue that we satisfy agreement when the sender is faulty. Assume now the sender is faulty. Since f=1, all non-senders are therefore honest. All non-senders will therefore broadcast the message that they have received from the sender in round 1. In round 3 all honest nodes have therefore received exactly the same set of messages. The outcome of their majority votes will therefore all be equal, and so they will all output the same value.\n\n\n\n\n\n\n\n\n\nExercise*\n\n\n\nUnder assumptions 1,2,3, f=2, and n \\geq 4 even, the protocol Alg_Cross_Check does not solve the Byzantine broadcast problem.\n\n\nShowing that the protocol breaks when f=2 is a bit tricky (hence the * next to the exercise). You are recommended to try to solve the exercise on your own, but don’t worry if you don’t succeed. In that case, please do take a look at the solution before proceeding to the next section.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nAssume there are 2 Byzantine nodes (faulty nodes). We show that they can coordinate in such a way that the protocol fails to satisfy agreement.\nAssume the two Byzantine nodes are nodes A and B and assume node A is the sender. Divide the remaining n-2 honest nodes into two groups, G_0 and G_1 of equal size (here we use that n is even).\nIn the first round, A sends message 0 to all nodes in G_0 and message 1 to all nodes in G_1. Also in the first round, A sends both messages 0 and 1 to B.2\nIn the second round, node B sends the signed message 0 that it received from A in round 1 to all nodes in G_0, and similarly sends the signed message 1 to all nodes in G_1.\nWhat do the honest nodes output in round 3? Consider a node i in G_0. It has received the message 0 a total of n/2 times: once from A, once from B, and once from the other n/2-2 nodes in G_0. It has received the message 1 a total of n/2-1 times: once from each of the nodes in G_1. Node i will therefore output 0 in round 3. Swapping the roles of 0 and 1 shows that a node j in G_1 will output the message 1 in round 3. We therefore violate agreement!\n\n\n\nA key takeaway from the above two exercises is that one round of cross-checking allows us to deal with one Byzantine node, but not with 2. It thus seems that more cross-checking is necessary when there are multiple Byzantine nodes. In a nutshell this is precisely what the Dolev-Strong protocol does: every additional round of cross-checking allows for one more Byzantine node.",
    "crumbs": [
      "Computer Science",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Dolev-Strong protocol</span>"
    ]
  },
  {
    "objectID": "DolevStrong.html#sec-DolevStrong_protocol",
    "href": "DolevStrong.html#sec-DolevStrong_protocol",
    "title": "4  The Dolev-Strong protocol",
    "section": "4.5 The Dolev-Strong protocol",
    "text": "4.5 The Dolev-Strong protocol\nHere we describe a classic protocol due to Dolev and Strong (1983, TODO: add ref). It solves the Byzantine broadcast problem in the permissioned and synchronous setting, assuming an upper bound f on the number of faulty nodes. Together with the reduction from Section 4.3, this solves the SMR problem under the same assumptions. To describe the protocol, we need one more definition, that of convincing messages.\n\n4.5.1 Convincing messages\nA node i is convinced of value v in round t if it receives a message prior to round t that satisfies the following three conditions:\n\nIt contains the value v;\nIt is first signed by the sender;\nIt is also signed by at least t-1 other, distinct nodes, none of which are i.\n\n\n\n4.5.2 Protocol description\nThe Dolev-Strong protocol(f): \n\n1.  In round 0 the sender:\n    Sends its private value v* to all the non-senders\n    Outputs v*\n2.  In round t = 1,...,f+1 a non-sender i does:\n        If i is convinced of a value v by some message m received \n        prior to round t and has not been convinced of v before: \n            i adds its signature to m and sends it to all non-senders\n3.  At the end of round f+1 a non-sender i does:\n        If i is convinced of exactly one value v: \n            Output v\n        Else:\n            Output \"failure\" (some message not in V)\nIn the above protocol we are outputting “failure” to signal that we have detected a Byzantine sender. The precise message “failure” is of course arbitrary, what matters is that it cannot be confused with a valid private input of the sender. The message “failure” is assumed to be distinguishable from inputs from V.\nIntuitively, in the Dolev-Strong protocol the f+1 rounds after the first correspond to f+1 rounds of cross-checking. Note that for f=1 the algorithm is not equal to Alg_Cross_Check. As we will see in the next section, the list of distinct signatures is used to detect Byzantine senders. For now, remark that a node that is newly convinced of a message at the end of round f+1 observes precisely f+1 distinct signatures, at least one more than the number of Byzantine nodes.\n\n\n4.5.3 Proof of correctness\nWe need to show that under the assumptions 1,2,3 the Dolev-Strong protocol satisfies termination, validity, and agreement. The property termination is clear from the description of the protocol. We prove the remaining two properties separately.\n\nLemma 4.2 Under the assumptions 1,2,3, assuming at most f faulty nodes, the Dolev-Strong protocol satisfies validity.\n\n\nProof. Assume that the sender is honest (why are we allowed to do so?) and has private value v^*. The sender thus follows the protocol and sends a signed copy of v^* to all non-senders in the first round, it then outputs v^* and terminates. In round 1 all non-senders are therefore convinced of the value v^*. Indeed, they have received a message that 1) contains the value v^*, 2) is first signed by the sender, and 3) is signed by 1-1=0 other, distinct nodes, none of which are i.\nIt remains to observe that since the sender is honest, no node is ever convinced of a value other than v^*: a convincing message needs to contain the signature of the sender. (Here we are using our assumption that signatures cannot be forged.)\nAt the end of the protocol, all non-senders therefore output v^* as well, establishing validity.\n\nThe hard(er) part is to show that the protocol satisfies agreement, and this is where we need the multiple rounds.\n\nLemma 4.3 Under the assumptions 1,2,3, assuming at most f faulty nodes, the Dolev-Strong protocol satisfies agreement.\n\n\nProof. Assume that the sender is Byzantine (why are we allowed to do so?). We will show that at the end of the protocol, all honest nodes are convinced of exactly the same set of values. This suffices to establish agreement (can you see why?3).\nSuppose on an honest node i gets newly convinced of a value v by a message received before the end of time step t for some t \\in \\{0,1,2,\\ldots, f+1\\}. We show that all other honest nodes also get convinced of value v before the end of the protocol. Now we need to distinguish two cases: i) t \\leq f or ii) t = f+1.\nCase i): if t \\leq f, then node i still has time to communicate to other honest nodes. In particular, in round t+1 it will add its signature to the message that convinced them of value v and it will send that to all other non-senders. Since node i was convinced of v in round t, this newly signed message is a convincing message for all other nodes that had not yet been convinced of v (check this!).\nCase ii): if t=f+1, then node i no longer has time to communicate. We thus need to show that all other honest nodes had already been convinced of the value v prior to the end of round t. To do so, we will crucially use that node i is convinced for the first time of value v at the end of round f+1. This means that at the end of round f+1 it receives a message that is signed first by the (Byzantine) sender, and also by f distinct other nodes (f = t-1). Crucially, since the sender is Byzantine, at least one of these f nodes is an honest node. Let j be such an honest node. Since i received a message containing the value v and j’s signature, the honest node j was newly convinced of the value v in some round t' \\leq f.4 We can thus apply case i) to the honest node j, showing that all non-sender nodes have received a convincing message containing the value v.",
    "crumbs": [
      "Computer Science",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Dolev-Strong protocol</span>"
    ]
  },
  {
    "objectID": "DolevStrong.html#footnotes",
    "href": "DolevStrong.html#footnotes",
    "title": "4  The Dolev-Strong protocol",
    "section": "",
    "text": "Suppose there is one faulty node. In some round, it will be elected as the leader. It can then simply choose to violate consistency by sending different messages to different nodes.↩︎\nSending both 0 and 1 to B is a bit artificial at this moment; we do so to cover the case where the signature scheme adds a signature that not only identifies the sender, but also the round in which the message was sent.↩︎\nEither every honest node is convinced of exactly one value v, in which case all honest nodes output v. Alternatively, every honest node is convinced of at least two distinct values, in which case all honest nodes output “failure”.↩︎\nIn fact, t'=f is the only possibility (since node i is only convinced in round f+1).↩︎",
    "crumbs": [
      "Computer Science",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Dolev-Strong protocol</span>"
    ]
  }
]